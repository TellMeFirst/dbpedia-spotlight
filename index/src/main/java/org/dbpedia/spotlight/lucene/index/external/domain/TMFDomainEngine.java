package org.dbpedia.spotlight.lucene.index.external.domain;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.hp.hpl.jena.rdf.model.Model;
import com.hp.hpl.jena.rdf.model.ModelFactory;
import org.dbpedia.spotlight.LDRClient;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

import java.io.*;
import java.nio.charset.StandardCharsets;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.nio.file.StandardOpenOption;
import java.util.*;

/**
 *
 * TODO --> Description of the class
 *
 */
public class TMFDomainEngine {

    final static Log LOG = LogFactory.getLog(TMFDomainEngine.class);

    public static void main(String[] args) throws Exception {

        LOG.info("Start to get domain entities...");

        Properties config = new Properties();
        String confFile = args[0];
        config.load(new FileInputStream(new File(confFile)));
        String domainConfPath = config.getProperty("tellmefirst.domain.conf", "").trim();
        String domainLoggerPath = config.getProperty("tellmefirst.domain.logger", "").trim();
        String domainProcessingPath = config.getProperty("tellmefirst.domain.processing", "").trim();
        String domainRDFPath = config.getProperty("tellmefirst.domain.RDF", "").trim();
        String domainEntitiesPath = config.getProperty("org.dbpedia.spotlight.data.conceptURIs", "").trim();
        String ldrEndpoint = config.getProperty("tellmefirst.domain.LDRService").trim();

        JSONReader jsonReader = new JSONReader(domainConfPath);
        String json = jsonReader.readJSON();
        ObjectMapper mapper = new ObjectMapper();
        JsonNode root = mapper.readValue(json, JsonNode.class);

        BufferedWriter logger;
        Path dstLogger = Paths.get(domainLoggerPath);
        logger = Files.newBufferedWriter(dstLogger, StandardCharsets.UTF_8);

        // Set objects to get domain entities
        TMFDomainQuery tmfDomainQuery = new TMFDomainQuery();
        LDRClient ldrClient = new LDRClient(ldrEndpoint);
        TMFDomainMerger tmfDomainMerger = new TMFDomainMerger();

        for(JsonNode node : root) {

            // Check if the domain file generated by a specific query exists
            String path = domainProcessingPath + "/" + getValue("description", node)
                                                          .replaceAll(" ", "_")
                                                          .toLowerCase();
            File f = new File(path);

            if(!f.exists()) {
                BufferedWriter domainWriter;
                Path dst = Paths.get(path);
                domainWriter = Files.newBufferedWriter(dst, StandardCharsets.UTF_8);

                logger.write("Query description: " + getValue("description", node));
                LOG.info("Query description: " + getValue("description", node));
                logger.newLine();

                logger.write("Language: " + getValue("language", node));
                LOG.info("Language: " + getValue("language", node));
                logger.newLine();

                LOG.info("Performing SPARQL query..");

                getEntitiesWithSPARQL(node, tmfDomainQuery, domainWriter, logger);

                domainWriter.close();

            } else {
                logger.write(getValue("description", node) + " has already been performed!");
                LOG.info(getValue("description", node) + " has already been performed!");
                logger.newLine();
            }
        }

        // TODO Add log related to the entities removed

        tmfDomainMerger.merge(domainProcessingPath, domainEntitiesPath);

        logger.write("Get new entities with the Linked Data Recommender...");
        LOG.info("Get new entities with the Linked Data Recommender...");
        logger.newLine();

        getDomainEntities(ldrClient, domainEntitiesPath, root.get(0), domainEntitiesPath);

        logger.write("Get RDF representation of entities...");
        LOG.info("Get RDF representation of entities...");
        logger.newLine();

        Model model = getRDF(tmfDomainQuery, domainEntitiesPath, root.get(0), domainRDFPath);
        System.out.println(model.toString());

        logger.close();
    }

    private static void getEntitiesWithSPARQL (JsonNode node, TMFDomainQuery tmfDomainQuery, BufferedWriter writer, BufferedWriter logger) throws IOException {
        int offset = 0;
        int appended = 0;
        int totalAppended = 0;

        List entities = tmfDomainQuery.getEntities(getValue("query", node), getValue("endpoint", node), offset);
        appended = appendEntities(writer, entities, getValue("baseuri", node)); // Do it at least once

        while (appended != 0) {
            // TODO Add try/catch
            List nextEntities = tmfDomainQuery.getEntities(getValue("query", node), getValue("endpoint", node), offset);
            appended = appendEntities(writer, nextEntities, getValue("baseuri", node));
            totalAppended += appended;
            offset += 10000; // This is the default number of rows returned by online endpoints
        }

        logger.write("Number of entities retrieved with query: " + totalAppended);
        LOG.info("Number of entities retrieved with query: " + totalAppended + "\n\n");
        logger.newLine();
    }

    private static void getDomainEntities(LDRClient ldrClient, String filename, JsonNode node, String domainEntitiesPath) throws Exception {
        BufferedReader reader = new BufferedReader(new FileReader(filename));
        String baseuri = getValue("baseuri", node);
        String[] clientParameters = new String[2];
        clientParameters[0] = baseuri;
        String uri = reader.readLine();

        while (uri != null) {
            clientParameters[1] = uri;
            List domainEntities = ldrClient.getDomainEntities(clientParameters);
            Iterator<String> iterator = domainEntities.iterator();
            while (iterator.hasNext()) {
                FileWriter writer = new FileWriter(domainEntitiesPath, true);
                writer.write(iterator.next() + "\n");
                writer.close();
            }
            uri = reader.readLine();
        }
        reader.close();
    }

    private static Model getRDF(TMFDomainQuery tmfDomainQuery, String entitiesListPath, JsonNode node, String domainRDFPath) throws IOException {
        Model model = ModelFactory.createDefaultModel();
        int processedUris = 0;
        String endpoint = getValue("endpoint", node);
        String baseuri = getValue("baseuri", node);

        BufferedReader reader = new BufferedReader(new FileReader(entitiesListPath));
        String entity;
        while(true) {
            entity = reader.readLine();
            if(entity == null)
                break;
            model.add(tmfDomainQuery.getRDFRepresentation(entity, baseuri, endpoint));
            processedUris++;
            if (processedUris %20000 == 0) {
                LOG.info("Publish RDF...");
                tmfDomainQuery.publishRDFOnFileSystem(model, domainRDFPath);
                model = ModelFactory.createDefaultModel();
            }
        }
        reader.close();
        return model;
    }

    private static int appendEntities(BufferedWriter writer,
                                      List entitiesList,
                                      String baseuri) throws IOException {
        int appended = 0;
        Iterator<String> iterator = entitiesList.iterator();
        while (iterator.hasNext()) {
            String entity = iterator.next();
            writer.write(entity.split(baseuri)[1]);
            writer.newLine();
            appended += 1;
        }
        return appended;
    }

    private static String getValue (String string, JsonNode record) {
        return record.get(string) != null ? record.get(string).asText() : "";
    }
}
